# -*- coding: utf-8 -*-
"""ising_nn_v8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iGraiW3KXsqwb8Jbvmnk5BclwoCsnzYX

#Ising Model CNN - Regression

##Libs
"""

# !pip install -U fastai &> /dev/null
# !!pip install pytorch_ranger &> /dev/null

# Commented out IPython magic to ensure Python compatibility.
#basic libraries
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import Normalize
from matplotlib import cm
from matplotlib.colors import LinearSegmentedColormap
import datetime
import hiddenlayer as hl
import pickle
import IPython
from torchviz import make_dot
from graphviz import Source
import graphviz
import scipy.misc
from PIL import Image
import json
#machine learning libraries
import torch
import torchvision.models as models
import torch.nn.functional as F
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torch.utils.data import random_split
from torch.nn.modules import conv
from torch.nn.modules.utils import _pair

#ranger optimizer #https://github.com/mpariente/Ranger-Deep-Learning-Optimizer
from pytorch_ranger import Ranger  
from pytorch_ranger import RangerVA
from pytorch_ranger import RangerQH

#fastai
# from fastai.data.core import Datasets
# from fastai.basics import *
# from fastai.vision.all import *
# from fastai.callback.all import *

#removing warnings
import warnings
warnings.filterwarnings("ignore")

#plot in jupyter notebook
# %matplotlib inline

#when using colab I like to do this to access the files in drive
#if running in a local pc: 
#           comment this cell
#           change the files' path
# from google.colab import drive
# drive.mount("/content/gdrive")

"""##Save Plots"""

#save figs
#save = 0         

#don't save figs
save = 1

"""##Seeds"""

np.random.seed(42)
torch.manual_seed(42)

"""##Functions

###Classes
"""

def create_csv(path:str=''):
    #path = r'TCC\Testes\testes20220907\q3-df80.gzip'

    L = 80
    df = pd.read_parquet(path)
    cols = list(df.columns[:-3])


    final_dict = {'rede':[],'temp':[]}

    for i in range(len(df)):
        # rede
        array = np.array(df.iloc[i][cols])
        array = np.reshape(array, (L,L))
        # temp
        temp = df.iloc[i]['T']


        final_dict['rede'].append(array)
        final_dict['temp'].append(temp)


    dff = pd.DataFrame(final_dict)

    #dff.to_csv(r'TCC\Testes\testes20220907\test.csv', index=False)

    #print('dff', dff.memory_usage(deep=True).sum()/10e5, 'MB', dff.shape)



    return dff

##dataset class
class ising_dataset(Dataset):
  def __init__(self, dataname):
    self.data   = dataname['rede']
    self.target = dataname['temp']

  def __len__(self):
    return len(self.data)

  def __getitem__(self, idx):
    current_sample = np.array(self.data[idx])
    current_sample = np.expand_dims(current_sample, axis=0)
    current_sample = current_sample.astype(np.float32)
    current_sample = torch.from_numpy(current_sample)
    current_target = self.target[idx]
    
    current_target = current_target.astype(np.float32)

    return torch.tensor(current_sample), torch.tensor(current_target)

"""###Functions"""

class Mish_layer(nn.Module):
  '''
  The class represents Mish activation function.
  '''
  def __init__(self):
    super(Mish_layer,self).__init__()

  def forward(self,x):
    return x*torch.tanh(F.softplus(x))

#randon split dataframe
def split_indexes(dataframe, train_size):
  num_train = len(dataframe)
  indices = list(range(num_train))

  split = int(np.floor(train_size * num_train))
  split2 = int(np.floor((train_size+(1-train_size)/2) * num_train))

  np.random.shuffle(indices)

  return indices[:split], indices[split2:], indices[split:split2]

"""###Xresnet"""

#Cell
def conv(n_inputs, n_filters, kernel_size=3, stride=1, bias=False) -> torch.nn.Conv2d:
    """Creates a convolution layer for `XResNet`."""
    return nn.Conv2d(n_inputs, n_filters, kernel_size=kernel_size, stride=stride, padding=kernel_size//2, bias=bias)

def conv_layer(n_inputs: int, n_filters: int, kernel_size: int = 3, stride=1,zero_batch_norm: bool = False,
               use_activation: bool = True):#, activation: torch.nn.Module = nn.ReLU(inplace=True)) -> torch.nn.Sequential:
    """Creates a convolution block for `XResNet`."""
    batch_norm = nn.BatchNorm2d(n_filters)
    activation = Mish_layer()

    # initializer batch normalization to 0 if its the final conv layer
    nn.init.constant_(batch_norm.weight, 0. if zero_batch_norm else 1.)
    layers = [conv(n_inputs, n_filters, kernel_size, stride=stride), batch_norm]
   
    if use_activation: 
        layers.append(activation)
    return nn.Sequential(*layers)

class XResNetBlock(nn.Module):
    """Creates the standard `XResNet` block."""
    def __init__(self, expansion: int, n_inputs: int, n_hidden: int, stride: int = 1):#,
                #  activation: torch.nn.Module = nn.ReLU(inplace=True)):
        super().__init__()
    
        activation = Mish_layer()

        n_inputs = n_inputs * expansion
        n_filters = n_hidden * expansion

        # convolution path
        if expansion == 1:
            layers = [conv_layer(n_inputs, n_hidden, 3, stride=stride),
                      conv_layer(n_hidden, n_filters, 3, zero_batch_norm=True, use_activation=False)]
        else:
            layers = [conv_layer(n_inputs, n_hidden, 1),
                      conv_layer(n_hidden, n_hidden, 3, stride=stride),
                      conv_layer(n_hidden, n_filters, 1, zero_batch_norm=True, use_activation=False)]

        self.convs = nn.Sequential(*layers)

        # identity path
        if n_inputs == n_filters:
            self.id_conv = nn.Identity()
        else:
            self.id_conv = conv_layer(n_inputs, n_filters, kernel_size=1, use_activation=False)
        if stride == 1:
            self.pool = nn.Identity()
        else:
            self.pool = nn.AvgPool2d(2, ceil_mode=True)

        self.activation = activation

    def forward(self, x):
        return self.activation(self.convs(x) + self.id_conv(self.pool(x)))

class XResNet(nn.Sequential):
    @classmethod
    def create(cls, expansion, layers, c_in=1, c_out=1):
        # create the stem of the network
        n_filters = [c_in, (c_in+1)*8, 64, 64]
        stem = [conv_layer(n_filters[i], n_filters[i+1], stride=2 if i==0 else 1)
                for i in range(3)]

        # create `XResNet` blocks
        n_filters = [64//expansion, 64, 128, 256, 512]

        res_layers = [cls._make_layer(expansion, n_filters[i], n_filters[i+1],
                                      n_blocks=l, stride=1 if i==0 else 2)
                      for i, l in enumerate(layers)]

        # putting it all together
        x_res_net = cls(*stem, nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
                        *res_layers, nn.AdaptiveAvgPool2d(1), nn.Flatten(),
                        nn.Linear(n_filters[-1]*expansion, c_out)
                       )

        cls._init_module(x_res_net)
        return x_res_net

    @staticmethod
    def _make_layer(expansion, n_inputs, n_filters, n_blocks, stride):
        return nn.Sequential(
            *[XResNetBlock(expansion, n_inputs if i==0 else n_filters, n_filters, stride if i==0 else 1)
              for i in range(n_blocks)])

    @staticmethod
    def _init_module(module):
        if getattr(module, 'bias', None) is not None:
            nn.init.constant_(module.bias, 0)
        if isinstance(module, (nn.Conv2d,nn.Linear)):
            nn.init.kaiming_normal_(module.weight)
        # initialize recursively
        for l in module.children():
            XResNet._init_module(l)

def xresnet18 (**kwargs): return XResNet.create(1, [2, 2,  2, 2], **kwargs)
def xresnet34 (**kwargs): return XResNet.create(1, [3, 4,  6, 3], **kwargs)
def xresnet50 (**kwargs): return XResNet.create(4, [3, 4,  6, 3], **kwargs)
def xresnet101(**kwargs): return XResNet.create(4, [3, 4, 23, 3], **kwargs)
def xresnet152(**kwargs): return XResNet.create(4, [3, 8, 36, 3], **kwargs)

def mse_loss_wgtd(pred, true, wgt=1.):
  loss = wgt*(pred-true).pow(2)
  return loss.mean()

def root_mean_squared_error(p, y): 
    return torch.sqrt(mse_loss_wgtd(p.view(-1), y.view(-1)))

def mae_loss_wgtd(pred, true, wgt=1.):
    loss = wgt*(pred-true).abs()
    return loss.mean()

def plot_dataset_temperature(dataset:DataLoader):
    temps = []
    i = 0
    for data in dataset:
        # if i < 10:
        #     print(data[1], type(data[1]), type(data[1].float()))
        for t in data[1].float():
            temps.append(t)

    #bins = np.linspace(0.494973, 1.484973, 10)

    plt.hist(temps)
    plt.show()
    return



def pipe(N          : int   = 18
        ,epochs     : int   = 10
        ,L          : int   = 80
        ,Q          : int   = 3
        ,train_size : float = 0.8
        ,batch_size : int   = 32
        ,root       : str   = ''
        ,save_nn    : bool  = False
        ,save_data  : bool  = False
        ,plots      : int   = 0 
        ,run_text   : str   = ''):

    if plots == 0:
        loss_plot = False
        results_plot = False
    elif plots == 1:
        loss_plot = True
        results_plot = False
    elif plots == 2:
        loss_plot = True
        results_plot = True
    else:
        loss_plot = False
        results_plot = False



    ROOT = r'D:\Backup\TCC\PKLS'
    DATE_HASH = datetime.datetime.today().strftime(r'%Y%m%d_%Hh%Mm')
    EXEC_HASH = f'N{N}_E{epochs}_Q{Q}_L{L}_' + DATE_HASH + f'B{batch_size}_{train_size}'

    #Data Loading
    pickle_path = ROOT + f"\q{Q}-df{L}.pkl"
    pickle_data = pd.read_pickle(pickle_path)

    #definitions
    L2 = L*L
    TEMP_MAX = max(pickle_data['temp'])
    TEMP_MIN = min(pickle_data['temp'])
    TC =  (1/(np.log(1+np.sqrt(Q))))
    



    # Train, Test, Validation SPLIT
    train_idx, test_idx, val_idx = split_indexes(pickle_data, train_size)

    data_train = ising_dataset(dataname=pickle_data.loc[train_idx].reset_index(drop=True))
    data_test = ising_dataset(dataname=pickle_data.loc[test_idx].reset_index(drop=True))
    data_val = ising_dataset(dataname=pickle_data.loc[val_idx].reset_index(drop=True))




    # print(f'data_train = {len(data_train)}')
    # print(f'data_test = {len(data_test)}')
    # print(f'data_val = {len(data_val)}')




    # device settings
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f'Device: {device}')



    # dataset loader
    train_dataloader = DataLoader(data_train, shuffle=True, batch_size=batch_size, num_workers=2)
    test_dataloader = DataLoader(data_test, shuffle=True, batch_size=batch_size, num_workers=2)
    val_dataloader = DataLoader(data_val, shuffle=True, batch_size=batch_size, num_workers=2)



    # clear cache
    torch.cuda.empty_cache

    # plot_dataset_temperature(train_dataloader)
    # plot_dataset_temperature(test_dataloader)
    # plot_dataset_temperature(val_dataloader)

    model_dict = {
         18  : xresnet18(c_in=1, c_out=1)
        ,34  : xresnet34(c_in=1, c_out=1)
        ,50  : xresnet50(c_in=1, c_out=1)
        ,101 : xresnet101(c_in=1, c_out=1)
        ,152 : xresnet152(c_in=1, c_out=1)
                }

    model = model_dict[N]


    mynet = model.to(device)
    loss_function = root_mean_squared_error
    optimizer = Ranger(mynet.parameters(), lr=1e-3) # colocando o mesmo lr do paper

    train_history = []

    mynet.train() 

    print(f'INICANDO {EXEC_HASH}')

    for epoch in range(0, epochs): # 5 epochs at maximum
        print(f'Starting epoch {epoch+1}', run_text)
        print(f'-----------------')
            
        train_loss = []

        for data in train_dataloader:
            inputs, targets = data[0].float(), data[1].float()
            inputs = inputs.to(device)
            targets = targets.to(device)
            outputs = mynet(inputs)
            
            loss = loss_function(outputs, targets)
            
            loss.backward()
            optimizer.step() 
            optimizer.zero_grad()

            train_loss.append(loss.item())

        train_loss = np.array(train_loss)
        train_history.append(train_loss.mean())

    print('Testing process has finished.')


    #save NN

    #load_nn = 0

    if save_nn == 1:
        nn_root = r'Testes\testes20221020'
        net_path = nn_root + f'weights_{EXEC_HASH}.pt'
        torch.save({'weights': mynet.state_dict()}, net_path)

    # if load_nn == 1:
    #     s = torch.load(net_path)
    #     mynet = mynet()
    #     mynet.load_state_dict(s['weights'])
    #     # mynet.state_dict()

    



    # LOSS PLOT
    fig, ax = plt.subplots(figsize=(16,9))
    x_axis = np.arange(0,epochs,1)
    plt.title(f'Loss x Epoch - {EXEC_HASH}', fontsize=30)
    plt.plot(x_axis, train_history, lw=3, label='Loss')
    plt.xlabel('Epochs', fontsize=15)
    plt.ylabel('Loss', fontsize=15)
    plt.legend(fontsize=15)
    if loss_plot:
        root_ = os.path.join(ROOT, 'plots')
        loss_plot_path = os.path.join(root_, f'loss_{EXEC_HASH}.jpeg')
        plt.savefig(loss_plot_path)


    # SAVE LOSS DATA
    if save_data:
        df_ = pd.DataFrame({'epochs':x_axis, 'loss':train_history})
        root_ = os.path.join(ROOT, 'data')
        df_path = os.path.join(root_, f'loss_{EXEC_HASH}.csv')
        df_.to_csv(df_path, index=False)



    mynet.eval()

    val_loss = []
    t_in = []
    t_out = []

    for data in val_dataloader:
        inputs, targets = data[0].float(), data[1].float()
        inputs = inputs.to(device)
        targets = targets.to(device)
        outputs = mynet(inputs)
    
        t_in.append(targets.cpu().detach().numpy())
        t_out.append(outputs.cpu().detach().numpy())
    

    loss = loss_function(outputs, targets)  
    
    val_loss.append(loss.item())

    val_loss = np.array(val_loss)
    val_loss = val_loss.mean()

    TEMPS_IN  = []
    TEMPS_OUT = []

    for item in range(len(t_in)):
        for i in t_in[item]:
            TEMPS_IN.append(i)

    for item in range(len(t_out)):
        for i in t_out[item]:
            TEMPS_OUT.append(i[0])

    print()
    print('Validation process has finished.')
    print(f'loss = ',val_loss)


    fig, ax = plt.subplots(figsize = (16,9))
    my_cmap = plt.get_cmap("rainbow")
    rescale = lambda y: (y - np.min(y)) / (np.max(y) - np.min(y))
    plt.title(f'Results - {EXEC_HASH}', fontsize=30)

    line = np.linspace(TEMP_MIN*0.83, TEMP_MAX*1.15, 200)


    #print('TEMPS_IN', len(TEMPS_IN))
    #print('TEMPS_OUT', len(TEMPS_OUT))
    # data points
    plot = ax.scatter(TEMPS_IN, TEMPS_OUT, c=TEMPS_OUT
                ,edgecolor='black', s=50, label=r'$T_{nn}$', cmap=my_cmap)
    fig.colorbar(plot)
    # Tc line
    plt.vlines(TC, ymin=TEMP_MIN*0.85, ymax=TEMP_MAX*1.15
                ,color='black', label=r'$T_{C} = $'+str(round(TC,4)), lw=3, ls='--')
    # T = T line
    plt.plot(line, line, color='black', label=r'$T_{R}=T_{nn}$', lw=3)

    plt.xlabel(r'$T_{R}$', fontsize=15)
    plt.ylabel(r'$T_{nn}$', fontsize=15)

    plt.xlim(TEMP_MIN*0.85,TEMP_MAX*1.15)
    plt.ylim(TEMP_MIN*0.85,TEMP_MAX*1.15)
    # fig.colorbar(cm.ScalarMappable(cmap=my_cmap)
    #             ,ax=ax
    #             ,values=np.linspace(TEMP_MIN, TEMP_MAX, 10)
    #             ,ticks=np.linspace(TEMP_MIN, TEMP_MAX, 10)
    #             ,drawedges=True)
    plt.legend(fontsize=15)
    if results_plot:  
        root_ = os.path.join(ROOT, 'plots')
        results_plot_path = os.path.join(root_, f'results_{EXEC_HASH}.jpeg')    
        plt.savefig(results_plot_path)
        

    #print(f'TEMPS_OUT', TEMPS_OUT)
    # SAVE TEMPS PLOT DATA
    if save_data:
        df_ = pd.DataFrame({'TR':[float(t) for t in TEMPS_IN]
                            ,'Tnn':[float(t) for t in TEMPS_OUT]})
        root_ = os.path.join(ROOT, 'data')
        df_path = os.path.join(root_, f'results_{EXEC_HASH}.csv')
        df_.to_csv(df_path, index=False)


    # NAO SEI OQ ACONTECE DAQUI PRA BAIXO
    for data in test_dataloader:
        inputs, targets = data[0].float(), data[1].float()
        inputs = inputs.to(device)
        targets = targets.to(device) 
        outputs = mynet(inputs)
        loss = loss_function(outputs, targets)
        
    #print(outputs.shape)
    # print(targets.shape)
            
    # plt.imshow(inputs[0][0].cpu())
    # plt.title(r'$T_p$ = ' + str(outputs[0][0].item()) + ' ; $T_r$ = ' + str(targets[0]))
    # plt.show()

    mydict = mynet.state_dict()
    cnn_w = mynet.state_dict().keys()
    check = []
    for item in cnn_w:
        b = np.array(mydict[item].cpu())
        if b.ndim == 4:
            check.append(item)

    kk = []
    for item in check:
        array = np.array(mydict[item].cpu())
        array = array[0,0,:,:]
        if array.shape == (1,1):
            pass#print(array.shape)
        else:
            kk.append(array)

    #print(len(kk))

    #three plots
    def multi_plot_snapshot(TEMPS_IN, TEMPS_OUT, cc, filename):

        #plot
        fig, axs = plt.subplots(1, 3, figsize=(15,14))
        
        axs[0].imshow(TEMPS_IN, cmap='Greys')
        axs[0].tick_params(left = False, right = False,
                            labelleft=False, labelbottom = False,
                            bottom = False)

        axs[1].imshow(cc, cmap='Greys')
        axs[1].tick_params(left = False, right = False,
                            labelleft=False, labelbottom = False,
                            bottom = False)

        axs[2].imshow(TEMPS_OUT, cmap='Greys')
        axs[2].tick_params(left = False, right = False,
                            labelleft=False, labelbottom = False,
                            bottom = False)
        plt.show()

        return

    return


def get_model(N:int=18):
    model_dict = {
         18  : xresnet18(c_in=1, c_out=1)
        ,34  : xresnet34(c_in=1, c_out=1)
        ,50  : xresnet50(c_in=1, c_out=1)
        ,101 : xresnet101(c_in=1, c_out=1)
        ,152 : xresnet152(c_in=1, c_out=1)
                }
    return model_dict[N]


def get_first_batch(Q, L, max_index:int=0):
    ROOT = r'D:\Backup\TCC\PKLS'
    #Data Loading
    pickle_path = ROOT + f"\q{Q}-df{L}.pkl"
    pickle_data = pd.read_pickle(pickle_path)

    # Train, Test, Validation SPLIT
    train_idx, test_idx, val_idx = split_indexes(pickle_data, 0.5)

    data_train = ising_dataset(dataname=pickle_data.loc[train_idx].reset_index(drop=True))

    # dataset loader
    train_dataloader = DataLoader(data_train, shuffle=True, batch_size=64, num_workers=2)


    if max_index > len(data_train):
        return 'Error'
    i = 0 
    max_ = max_index
    for data in train_dataloader:
            if i < max_:
                inputs, temps = data[0].float(), data[1].float()
                i+=1
            else:
                break
    return inputs, temps


def visualize_pipe_convs(model, Q, L, hash_):

    # we will save the conv layer weights in this list
    model_weights =[]
    #we will save the 49 conv layers in this list
    conv_layers = []    # get all the model children as list
    x = list(model.modules())#counter to keep count of the conv layers
    counter = 0#append all the conv layers and their respective wights to the list
    model_weights = []
    #we will save the 49 conv layers in this list

    total_layers = []
    counter = 0
    blocks = 0
    convs = 0

    last_in_channel = 0


    for mod in list(model.modules()):
        if isinstance(mod, XResNetBlock):
            blocks+=1
            #print('res')
            #rint(mod)
        elif isinstance(mod, nn.Conv2d):
            actual_channels = mod.in_channels     
            if last_in_channel > actual_channels:
                pass
                #print(last_in_channel, actual_channels)
                #print(convs)
                #print(convs_layers[convs-2: convs])
            else:
                conv_layers.append(mod)
                counter+=1
            last_in_channel = actual_channels

                    
    print('conv layers:', counter)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(device)
    model = model.to(device)

    image, temps = get_first_batch(Q, L, max_index=25)

    image_aux = image.to(device)
    
    outputs = []
    names = [f'original image T={round(float(temps[0]), 4)}']
    for layer in conv_layers[0:]:
        image_aux = layer(image_aux)
        image_aux = image_aux.to(device)
        outputs.append(image_aux)
        names.append(str(layer))
    #print(len(outputs))  #print feature_maps
    # for feature_map in outputs:
    #     print(feature_map.shape)

    processed = []
    for feature_map in outputs:
        feature_map = feature_map[0].squeeze(0)
        gray_scale = torch.sum(feature_map,0)
        gray_scale = gray_scale / feature_map.shape[0]
        processed.append(gray_scale.data.cpu().numpy())
    # for fm in processed:
    #     print(fm.shape)


    fig = plt.figure(figsize=(30, 50), dpi=120)
    a = fig.add_subplot(5,4, 1)
    imgplot = plt.imshow(image[0][0])
    a.set_title(names[0], fontsize=30)
    infos_teste = {}
    infos_teste[0]= {'soma': np.sum(image[0][0].cpu().numpy())
                        , 'media': np.mean(image[0][0].cpu().numpy())
                        , 'std': np.std(image[0][0].cpu().numpy())
                        , 'rede':image[0][0].cpu().numpy()
                        , 'shape':image[0][0].shape
                        , 'temp': round(float(temps[0]), 4)}
    

    for i in range(len(processed)):
        a = fig.add_subplot(5, 4, i+2)
        imgplot = plt.imshow(processed[i])
        infos_teste[i+1]= {'soma': np.sum(processed[i])
                        , 'media': np.mean(processed[i])
                        , 'std': np.std(processed[i])
                        , 'rede':processed[i]
                        , 'shape':processed[i].shape
                        , 'temp': round(float(temps[0]), 4)}
        #a.axis("off")
        #print(names[i+1])
        name = names[i+1].split(',')[0] +', '+names[i+1].split(',')[1] +')'
        a.set_title(name, fontsize=30)
        path_ = r'Testes\testes20221020\graficos\feature_maps'
        filename = str('feature_maps_v3_'+hash_+'.jpg')
        final_path = os.path.join(path_, filename)
    plt.savefig(final_path, bbox_inches='tight')
    print(hash_, 'finalizada')
    return  infos_teste
# from scipy import ndimage

# N = 0

# TEMPS_IN = pickle_data.iloc[N]['SPINS']
# temp = pickle_data.iloc[N]['T']

# TEMPS_OUT = ndimage.convolve(TEMPS_IN, kk[0], mode='constant', cval=0.0)

# print(temp)
# # multi_plot_snapshot(TEMPS_IN,TEMPS_OUT,'teste.dat')

# for i in kk:
#     TEMPS_OUT = ndimage.convolve(TEMPS_IN, i, mode='constant', cval=0.0)
#     multi_plot_snapshot(TEMPS_IN,TEMPS_OUT,i,'teste.dat')

# fig, ax = plt.subplots(figsize=(12, 12))
# plt.imshow(kk[0], cmap='Greys')




if __name__ == '__main__':

    # li = [1,2,3,4,5]
    # print(li)

    # print(li[0:])
    
    # for q in [2]:
    #     pipe(
    #         Q=q
    #         ,plots=2, save_data=True, epochs=2
    #         , batch_size=64, N=18, save_nn=True, L=120
    #         )
    # pipe(Q=2, N=34, epochs=1, L=120, save_data=True, plots=2,
    #         train_size=0.5, batch_size=64)



    # path = r'Testes\testes20221020\nn\weights_N18_E100_Q2_L120_20221115_20h31mB64_0.5.pt'
    # model = get_model(18)
    # model.load_state_dict(torch.load(path), strict=False)
    # model.eval()
    # INFOS = {}
    # for k in range(5):
    #     hash_ = f'run {k+1}_nov17_v2'
    #     INFOS[k] = visualize_pipe_convs(model, 2, 120, hash_)


    path_ = r'Testes\testes20221020\graficos\feature_maps'
    filename = 'infos_test5_q2.pkl'
    final_path = os.path.join(path_, filename)
    # with open(final_path, 'wb') as file:
    #     pickle.dump(INFOS, file,  protocol=-1)

    with open(final_path, 'rb') as file:
        infos = pickle.load(file)

    #print(infos)
   
    # print(infos.keys())
    #fig = plt.figure(figsize=(30, 50), dpi=120)

    inf1 = infos[0]
    inf2 = infos[1]
    inf3 = infos[2]
    inf4 = infos[3]
    inf5 = infos[4]

    # bins = 15
    # #, k2, k3, k4, k5
    # for k1 in inf1.keys():#, inf2.keys(), inf3.keys(), inf4.keys(), inf5.keys()):
    #     #print(inf[k]['shape'])
    #     # print(list(inf[k]['rede'].values))
    #     # print([val for val in inf[k]['rede'].flatten()])
    #     a = fig.add_subplot(5, 4, k1+1)
    #     # k1
    #     imgplot = plt.hist(list(inf1[k1]['rede'].flatten())
    #                     , density=True, stacked=True
    #               #      , color=cm.rainbow(inf1[k1]['temp'])
    #                     , bins = bins
    #                     ,label = f"T = {inf1[k1]['temp']}")    
    #     # k2
    #     imgplot = plt.hist(list(inf2[k1]['rede'].flatten())
    #             , density=True, stacked=True
    #            # , color=cm.rainbow(inf2[k1]['temp'])
    #             , bins =bins
    #             , label = f"T = {inf2[k1]['temp']}") 
    #     # k3
    #     # imgplot = plt.hist(list(inf3[k1]['rede'].flatten())
    #     #         , density=True, stacked=True
    #     #         , color=cm.rainbow(inf3[k1]['temp'])
    #     #         , bins =bins
    #     #         ,label=f"T = {inf3[k1]['temp']}") 
    #     # k4
    #     imgplot = plt.hist(list(inf4[k1]['rede'].flatten())
    #             , density=True, stacked=True
    #            # , color=cm.rainbow(inf4[k1]['temp'])
    #             , bins =bins
    #             ,label=f"T = {inf4[k1]['temp']}") 
    #     # k5
    #     # imgplot = plt.hist(list(inf5[k1]['rede'].flatten())
    #     #         , density=True, stacked=True
    #     #         , color=cm.rainbow(inf5[k1]['temp'])
    #     #         , bins =bins
    #     #         ,label=f"T = {inf5[k1]['temp']}") 
    #     a.set_title(f"layer {k1+1} {inf1[k1]['shape']}", fontsize=30)
    #     a.legend()
    #     #print(k1)
    # # # print('temp1', inf1[k1]['temp'])
    # # # print('temp2', inf2[k1]['temp'])
    # # # print('temp3', inf3[k1]['temp'])
    # plt.savefig(r'Testes\testes20221020\graficos\feature_maps\test_hist_conj5_q2.png')
    # #plt.show()
    



    fig, ax = plt.subplots(figsize=(16,9))
    my_cmap = plt.get_cmap("rainbow")
    #plt.title(')
    for k in infos.keys():
        inf = infos[k].copy()
        camadas = list(inf.keys())
        medias = [inf[i]['media'] for i in camadas ]
        medias = [inf[i]['std'] for i in camadas ]
        sizes = [inf[i]['rede'].shape for i in camadas ]
        temp = inf[camadas[0]]['temp']

        
        
        
        ax.plot(camadas, medias, label=temp, c=cm.rainbow(temp), lw=2)
        
    #plot = ax.scatter(camadas, medias, c=[temp for _ in range(len(camadas))], cmap=my_cmap, s=1)
    #plt.xticks(camadas)
    #fig.colorbar(cm.ScalarMappable(norm=norm, cmap=my_cmap), ax=ax)
    #fig.colorbar(plot, label='T')
    plt.legend()
    plt.show()








    # x, y = get_first_batch(6,120, 15)
    # print(y.shape)
    # print(round(float(y[0]), 4))
    # print(x[0].shape)
    # print(x[0][0].shape)
    # print(x[0][0])
    # # 
    # #
    # with open(path, 'rb') as pkl:
    #     x = pickle.load(pkl)

    # # print(x)
    # blocks = 0
    # convs = 0
    # in_channels = []
    # ok_channels = []
    # last_in_channel = 0
    # convs_layers = []

    # for mod in list(model.modules()):
    #     if isinstance(mod, XResNetBlock):
    #         blocks+=1
    #         #print('res')
    #         #rint(mod)
    #     elif isinstance(mod, nn.Conv2d):
    #         actual_channels = mod.in_channels     
    #         if last_in_channel > actual_channels:
    #             print(last_in_channel, actual_channels)
    #             #print(convs)
    #             #print(convs_layers[convs-2: convs])
    #         else:
    #             convs_layers.append(mod)
    #             convs+=1
    #             ok_channels.append(actual_channels)
    #             in_channels.append(actual_channels)
    #         last_in_channel = actual_channels
    # print('blocks', blocks)
    # print('convs', convs)
    # print(in_channels)
    # print(ok_channels)


        #else:
           # print(type(mod))
    # t = torch.tensor([[[1, 2],
    #                [3, 4]],
    #               [[5, 6],
    #                [7, 8]],
    #                [[9,10],
    #                [12,12]]])
    # print(t)
    # print(t.shape)
    # print(t[0].shape)
    # print(t[1].shape)
    # print(t[2].shape)
    # print(t[5].shape)
    #print(torch.flatten(t))

    #print(torch.flatten(t, start_dim=1))












        # for j in range(len(x)):
    #     lay = x[j]
    #     #print(lay, type(lay))
    #     if type(lay) == nn.Sequential:
    #         #print('seq')
    #         for k in lay:
    #             if isinstance(k, XResNetBlock):
    #                 #print('res')
    #                 for k2 in list(k.children()):

    #                     if type(k2) == nn.Sequential:

    #                         for k3 in list(k2.children()):

    #                             if type(k3) ==  nn.Sequential:

    #                                 for k4 in list(k3.children()):
    #                                     if type(k4) == nn.Conv2d:
    #                                         counter+=1
    #                                         #model_weights.append(k4.weight)
    #                                         #conv_layers.append(k4)
    #                                         #print('k', k4)

    #                             elif type(k3) == nn.Conv2d:
    #                                 counter+=1
    #                                 #model_weights.append(k3.weight)
    #                                 #conv_layers.append(k3)
    #                                 #print('k', k3)
    #                     elif type(k2) == nn.Conv2d:
    #                         counter+=1
    #                         #model_weights.append(k2.weight)
    #                         #conv_layers.append(k2)
    #                         #print('k', k2)
    #             else:
    #                 if type(k) == nn.Conv2d:
    #                     counter+=1
    #                     model_weights.append(k.weight)
    #                     conv_layers.append(k)



    # ROOT = r'D:\Backup\TCC\PKLS'
 
    # #Data Loading
    # pickle_path = ROOT + f"\q{4}-df{120}.pkl"
    # pickle_data = pd.read_pickle(pickle_path)

    



    # # Train, Test, Validation SPLIT
    # train_idx, test_idx, val_idx = split_indexes(pickle_data, 0.5)

    # data_train = ising_dataset(dataname=pickle_data.loc[train_idx].reset_index(drop=True))
    # data_test = ising_dataset(dataname=pickle_data.loc[test_idx].reset_index(drop=True))
    # data_val = ising_dataset(dataname=pickle_data.loc[val_idx].reset_index(drop=True))

    # # device settings
    # device = 'cuda' if torch.cuda.is_available() else 'cpu'
    # print(f'Device: {device}')

    # # dataset loader
    # train_dataloader = DataLoader(data_train, shuffle=True, batch_size=64, num_workers=2)

    # for data in train_dataloader:
    #     inputs, targets = data[0].float(), data[1].float()

    #     batch = inputs
    #     #print(batch)
        
    #     break
    
    # out = model(inputs)

    # make_dot(out).render('test', format='png')
        
    #yhat = model(batch) # Give dummy batch to forward().
    #model_arch = make_dot(yhat)

    #Source(model_arch).render('xresnet18_graph.png')
    #torch.onnx.export(model, batch, 'xresnet18.onnx')


    # graph = hl.build_graph(model, batch)
    # #graph.theme = hl.graph.THEMES['blue'].copy()
    # path = r'C:\Users\Mateus\Workspace\FisComps\TCC\Testes\testes20221020\plots\test.png'
    # graph.save(path, format='png')


    # runs = 5
    # for i in range(runs):
    #     pipe(Q=4, N=18, epochs=100, L=120, save_data=True, plots=2,
    #         train_size=0.5, run_text=f' [run {i+1}/{runs}]', batch_size=64)
    # runs = 20
    # for i in range(runs):
    #     pipe(Q=2, N=34, epochs=50, L=120, save_data=True, plots=2,
    #         train_size=0.5, run_text=f' [run {i+1}/{runs}]', batch_size=64)

    # for q in [3,4,7,8]:
    #     pipe(
    #         Q=q
    #         ,plots=2, save_data=True, epochs=50
    #         , batch_size=16, N=152
    #         )

    
  